{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23f3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d78ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45ccec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all variables \n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc02ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset file\n",
    "with open('C:/Users/Asus/OneDrive/Desktop/language-translator-ml-codes/eng-french.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.read().split('\\n')\n",
    "#read first 10,000 rows from dataset     \n",
    "for row in rows[:10000]:\n",
    "    #split input and target by '\\t'=tab\n",
    "    input_text,target_text = row.split('\\t')\n",
    "    #add '\\t' at start and '\\n' at end of text.\n",
    "    target_text='\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text.lower())\n",
    "    target_texts.append(target_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fdc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort input and target characters \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "#get the total length of input and target characters\n",
    "num_en_chars = len(input_characters)\n",
    "num_dec_chars = len(target_characters)\n",
    "#get the maximum length of input and target text.\n",
    "max_input_length = max([len(i) for i in input_texts])\n",
    "max_target_length = max([len(i) for i in target_texts])\n",
    "\n",
    "def bagofcharacters(input_texts,target_texts):\n",
    "  #inintialize encoder , decoder input and target data.\n",
    "  en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "  #padding variable with first character as 1 as rest all 0.\n",
    "  pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "  pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "  #countvectorizer for one hot encoding as we want to tokenize character so\n",
    "  #anlyzer is true and None the stopwords action.\n",
    "  cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')\n",
    "  for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "    #fit the input characters into the CountVectorizer function\n",
    "    cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "    #transform the input text from the help of CountVectorizer fit.\n",
    "    #it character present than put 1 and 0 otherwise.\n",
    "    en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "    cv_tar= cv.fit(target_characters)\t\t\n",
    "    dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "    #decoder target will be one timestep ahead because it will not consider \n",
    "    #the first character i.e. '\\t'.\n",
    "    dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "    \n",
    "    #add padding variable if the length of the input or target text is smaller\n",
    "    #than their respective maximum input or target length. \n",
    "    if len(input_t) < max_input_length:\n",
    "        for _ in range(max_input_length-len(input_t)):\n",
    "            en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_target_length:\n",
    "        for _ in range(max_target_length-len(target_t)):\n",
    "            dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_target_length:\n",
    "        for _ in range(max_target_length-len(target_t)+1):\n",
    "            dec_tr_data[i].append(pad_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc89de2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'en_in_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11324/2253736512.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#convert list to numpy array with data type float32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0men_in_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_in_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdec_in_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_in_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdec_tr_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_tr_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'en_in_data' is not defined"
     ]
    }
   ],
   "source": [
    "  #convert list to numpy array with data type float32\n",
    "en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "\n",
    "  return en_in_data,dec_in_data,dec_tr_data\n",
    "\n",
    "#create input object of total number of encoder characters\n",
    "en_inputs = Input(shape=(None, num_en_chars))\n",
    "#create LSTM with the hidden dimension of 256\n",
    "#return state=True as we don't want output sequence.\n",
    "encoder = LSTM(256, return_state=True)\n",
    "#discard encoder output and store hidden and cell state.\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]\n",
    "\n",
    "#create input object of total number of decoder characters\n",
    "dec_inputs = Input(shape=(None, num_dec_chars))\n",
    "#create LSTM with the hidden dimension of 256\n",
    "#return state and return sequences as we want output sequence.\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "#initialize the decoder model with the states on encoder.\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
    "#Output layer with shape of total number of decoder characters \n",
    "dec_dense = Dense(num_dec_chars, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "#create Model and store all variables \n",
    "model = Model([en_inputs, dec_inputs], dec_outputs)\n",
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters,\n",
    "             'max_input_length':max_input_length,'max_target_length':max_target_length,\n",
    "             'num_en_chars':num_en_chars,'num_dec_chars':num_dec_chars},open(\"training_data.pkl\",\"wb\"))\n",
    "\n",
    "#load the data and train the model\n",
    "en_in_data,dec_in_data,dec_tr_data = bagofcharacters(input_texts,target_texts)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")\n",
    "\n",
    "#summary and model plot\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e4b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
